"""
KNN算法介绍（K Nearest Neighbors），K近邻算法
    原理：
        基于欧式距离（或者其他距离计算方式）计算测试集和每个训练集之间的距离，然后根据距离升序排序，找到最近的K个样本
        基于K个样本投票，票数多的就作为最终预测结果 -> 分类问题，
        基于K个样本计算平均值，作为最终预测结果 ->回归问题
    实现思路：
        1. 分类问题
            适用于：有特征，有标签，且标签是不连续的（离散的）
        2. 回归问题
            适用于：有特征，有标签，且标签是连续的
    KNN算法回归问题思路如下：
        1. 计算测试集和每个训练的样本之间的距离
        2. 基于距离进行升序排列
        3. 找到最近的K个样本
        4. 基于K个样本的标签值，计算平均值
        5. 将上述计算出来的平均值，作为最终的预测结果
    代码实现思路：
        1. 导包
        2. 准备数据集（测试集和训练集）
        3. 创建（KNN分类模型）模型对象
        4. 模型训练
        5. 模型预测
    总结：
        K值过小，容易受到异常值的影响，且会导致模型学到大量脏特征，导致过拟合
        K值过大，模型会变得简单，容易发生欠拟合
"""
# 1. 导包
from sklearn.neighbors import KNeighborsRegressor  # KNN的回归模型

# 2. 准备数据集（测试集和训练集）
x_train = [[0, 0, 1], [1, 1, 0], [3, 10, 10], [4, 11, 12]]  # 训练集的特征数据
y_train = [0.1, 0.2, 0.3, 0.4]  # 训练集的标签数据
x_test = [[3, 11, 10]]  # 测试集的特征数据

# 3. 创建（KNN分类模型）模型对象
estimator = KNeighborsRegressor(n_neighbors=2)

# 4. 模型训练
estimator.fit(x_train, y_train)

# 5. 模型预测
y_pred = estimator.predict(x_test)

# 6. 打印预测结果
print(y_pred)
